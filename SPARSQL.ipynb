{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPARK SQL (CLOUDS Fall 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to use the DataFrame API and SparkSQL to perform simple data analytics tasks.\n",
    "\n",
    "# Goals\n",
    "The main goals of this notebook are the following:\n",
    "1. Understand the advantages and disadvantages of using DataFrame over RDD\n",
    "2. Analyze the airline data with the DataFrame API and SparkSQL\n",
    "\n",
    "# Steps\n",
    "\n",
    "- First, in section 1, we will go through a short introduction about the DataFrame API with a small example to see how can we use it and how it compares to the low-level RDD abstraction.\n",
    "- In section 2, we delve into the details of the use case of this notebook including: providing the context, introducing the data\n",
    "- In section 3, we perform data exploration and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RDD and DataFrame APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous notebooks, we have worked with RDDs (Resilient Distributed Dataset), the basic abstraction in Spark. The main reason is due to the task at hand: we have been invovled in the design of distributed algorithms.\n",
    "\n",
    "In this laboratory session, we study (or revisit) the DataFrame API - an immutable distributed collection of data.\n",
    "DataFrames allow developers to impose a **structure** on a distributed collection of data, allowing higher-level abstraction; in addition, DataFrames provide a domain specific language (DSL) API to manipulate distributed, structured data. Ultimately, the goal is to make Spark accessible to a wider audience, beyond researchers and specialized data engineers.\n",
    "\n",
    "Let's take a small example to see the difference between RDDs and DataFrames. The two cells below contain short code snippets that perform the same task: count the frequency of words in a document, then sort the result in descending order. The first snippet is the same you have been working on in the very [first notebook](https://github.com/EURECOM-CLOUDS-COURSE/Introduction/blob/master/introduction-lab.ipynb) of the CLOUDS course.\n",
    "The second code snippet uses the DataFrame API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<strong>Question 1.</strong> \n",
    "\n",
    "Run both approaches (RDD and DataFrame) detailed below, compare their runtime, and discuss. Is the DataFrame API approach faster? Can you explain why?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d0ea0a2efc18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# sort result by key DESC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m          )\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Runtime: %.2f sec\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m         \u001b[0mrddSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# empty RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \"\"\"\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \"\"\"\n\u001b[0;32m-> 1039\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    914\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "time_start = time.time()\n",
    "words = (\n",
    "            # read the text file\n",
    "            sc.textFile(\"/datasets/gutenberg/gutenberg_small.txt\").repartition(16)\n",
    "            \n",
    "            # construct words from lines\n",
    "            .flatMap(lambda line: line.split())\n",
    "            \n",
    "            # map each word to (word, 1)\n",
    "            .map(lambda x: (x, 1))\n",
    "    \n",
    "            # reduce by key: accumulate sum the freq of the same word\n",
    "            .reduceByKey(lambda freq1, freq2: freq1 + freq2)\n",
    "            \n",
    "            # swap (word, freq) to (freq, word)\n",
    "            .map(lambda x: (x[1], x[0]))\n",
    "    \n",
    "            # sort result by key DESC\n",
    "            .sortByKey(False)\n",
    "         )\n",
    "print(\"Runtime: %.2f sec\" % (time.time() - time_start))\n",
    "print(words.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import explode, split, desc\n",
    "sqlContext = SQLContext(sc)\n",
    "time_start = time.time()\n",
    "\n",
    "# read the text file\n",
    "df = sqlContext.read.text(\"/datasets/gutenberg/gutenberg_small.txt\")\n",
    "\n",
    "# show 3 samples\n",
    "# df.show(3)\n",
    "\n",
    "result = (\n",
    "    # split into words\n",
    "    df.select(explode(split(df.value, \"\\s+|\\t+\")).alias('word'))\n",
    "    \n",
    "    # group by each word\n",
    "    .groupBy(['word'])\n",
    "    \n",
    "    # count the freq\n",
    "    .count()\n",
    "    \n",
    "    # sort the result\n",
    "    .orderBy(desc(\"count\"))\n",
    ")\n",
    "\n",
    "result.show(5)\n",
    "print(\"Runtime: %.2f sec\" % (time.time() - time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of the above pieces of code work on the same task of word count and displaying the top 5 words, but the approach of using the DataFrame API (88.20 sec) achieved the task faster than the approach of using RDDs (134.87 sec).<br>\n",
    "This is because the SparkSQL and its utilization of the DataFrame API possesses features that enhance the efficiency and optimize the execution of the required operations of data. In SparkSQL, the <b>Catalyst Optimizer</b>, which is a component of SparkSQL, works on the optimization of the query implementation and processing. It optimizes the <b>logical plan</b> of the desired query by implementing several optimization rules on the <b>query tree</b> and also chooses the best <b>physical plan</b> of the query based on the cost model as well as code generation with <b>quasiquoutes</b>. So it has rule-based and cost-based optimization capabilities which is a big boost over the RDD approach implementation.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analysis of flight data using the DataFrame API and SparkSQL\n",
    "\n",
    "## Use case\n",
    "\n",
    "In this notebook, we play the role of a data scientist working in the travel industry, specifically on air transportation of passengers. We want to explore the data collected by the Department of Transportation (DoT) to understand passengers' behavior, as well as the properties of all flights, across several airline companies.\n",
    "\n",
    "The dataset provided by the DoT has 29 features, that can be either categorical or numerical. For example, the ```src_airport``` (source airport) is categorical: there exist no comparison operator between airport names. We can not say \"SGN is bigger than NCE\". The ```departure_time``` feature is numerical, for which a comparison operator exists. For instance, \"flight departing before 6PM\" can be express by \"departure_time < 1800\".\n",
    "\n",
    "In this use case, most features are numerical, except carrier, flight_number, cancelled, cancelation_code and diverted. \n",
    "The data contains a header, that is useless when analyzing the data: it serves the purpose of an \"embedded schema\", to help data scientist figure out what information is available. Note that there are some features with missing values in some lines of the dataset. The missing values are marked by \"NA\". These values can cause problems when processing the data and can lead to unexpected results. Therefore, we need to remove the header and replace all \"NA\" values by empty values, such as they can be interpreted as null values.\n",
    "\n",
    "## DataFrame\n",
    "As we have seen already, there are multiple ways to manipulate data:\n",
    "\n",
    "- Using the RDD abstraction\n",
    "\n",
    "- Using the DataFrame abstraction. \n",
    "\n",
    "A DataFrame is a distributed collection of data organized into named columns. It is based on the data frame concept in R language or in Pandas for Python. In some sense, it is similar to a table in a relational database: each item is a list of values (the columns). Also, the value in each row of each column can be accessed by the column's name.\n",
    "\n",
    "To use DataFrames, the data should be clean (no invalid values). That means we cannot create DataFrame directly from the \"RAW\" data. Instead, we will first create an RDD from RAW data, produce a new, clean RDD, then transform it to a DataFrame and work on it. The RDD `cleaned_data` is an `RDD[String]`. We need to transform it to `RDD[(TypeOfColumn1, TypeOfColumn2, ..., TypeOfColumn29)]` then call a function to create a DataFrame from the new RDD. Please note that, package `com.databricks.spark.csv` can really help us to load the data efficiently without concerns about parsing, conversions, etc... However, the CSV package can have some problems when loading not well-formatted data or the data that has many kinds of missing values. The guidelines defined this notebook are a general way to help you work with any data.\n",
    "\n",
    "\n",
    "DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n",
    "\n",
    "During the labs in this course, we will mainly work with CSV data files. So, in the next sections, we only focus on constructing dataframes from structured data files directly and from existing RDD.\n",
    "\n",
    "### Constructing a DataFrame directly from structured data file\n",
    "\n",
    "To construct DataFrame from a structured file directly, the file type must be supported. Currently, Spark supports csv, json, avro and many more. Among these types, the csv type is one of the most popular in data analytics. A DataFrame is constructed from csv files by using the package spark-csv from Databricks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-940f2f34f3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NA'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.load('/datasets/airline/1994.csv', \n",
    "    format='com.databricks.spark.csv', \n",
    "    header='true', \n",
    "    inferSchema='true',\n",
    "    nullValue='NA'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function load parametrized with `com.databricks.spark.csv`, we ask the SqlContext to use the parser from the DataBricks package. Additionally, we can specify whether the file has a header, or ask the parser to guess the data type of columns automatically. The parsed data types is viewed using the function printSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(df.dtypes)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the automatically inferred data types are **not** as expected. For example, we expect that `CRSDepTime` to be of interger type. The type and the name of each column can be modified using function ```withColumn``` and ```withColumnRename``` respectively. Additionally, we can also compute and print basic descriptive statistics of numerical columns via the function ```describe``` (similar to Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = (df\n",
    "          # change type of column CRSDepTime by casting its values to interger type\n",
    "          .withColumn('CRSDepTime', df.CRSDepTime.cast('int'))\n",
    "\n",
    "          # rename the column\n",
    "          .withColumnRenamed('CRSDepTime', 'scheduled_departure_time')\n",
    "    )\n",
    "\n",
    "# print schema of the current data\n",
    "df.printSchema()\n",
    "\n",
    "# run jobs to calculate basic statistic information and show it\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a DataFrame from an existing RDD\n",
    "\n",
    "Another way to construct a DataFrame is using data from an existing RDD. The main advantage of this approach is that it does not need a third party library. However, with this method, we have to remove the header ourself and provide a clear schema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import explode, split, desc, col, udf, round\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as func\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "data = sc.textFile('/datasets/airline/1994.csv')\n",
    "\n",
    "# extract the header\n",
    "header = data.first()\n",
    "\n",
    "# replace invalid data with NULL and remove header\n",
    "cleaned_data = (\n",
    "        data\n",
    "    \n",
    "        # filter out the header\n",
    "        .filter(lambda line: line != header)\n",
    "    \n",
    "         # replace the 'missing data' by empty value\n",
    "        .map(lambda line: ','.join(list(map(lambda x: x if x != 'NA' else '',line.split(',')))))\n",
    "    )\n",
    "\n",
    "airline_data_schema = StructType([ \\\n",
    "    #StructField( name, dataType, nullable)\n",
    "    StructField(\"year\",                     IntegerType(), True), \\\n",
    "    StructField(\"month\",                    IntegerType(), True), \\\n",
    "    StructField(\"day_of_month\",             IntegerType(), True), \\\n",
    "    StructField(\"day_of_week\",              IntegerType(), True), \\\n",
    "    StructField(\"departure_time\",           IntegerType(), True), \\\n",
    "    StructField(\"scheduled_departure_time\", IntegerType(), True), \\\n",
    "    StructField(\"arrival_time\",             IntegerType(), True), \\\n",
    "    StructField(\"scheduled_arrival_time\",   IntegerType(), True), \\\n",
    "    StructField(\"carrier\",                  StringType(),  True), \\\n",
    "    StructField(\"flight_number\",            StringType(),  True), \\\n",
    "    StructField(\"tail_number\",              StringType(),  True), \\\n",
    "    StructField(\"actual_elapsed_time\",      IntegerType(), True), \\\n",
    "    StructField(\"scheduled_elapsed_time\",   IntegerType(), True), \\\n",
    "    StructField(\"air_time\",                 IntegerType(), True), \\\n",
    "    StructField(\"arrival_delay\",            IntegerType(), True), \\\n",
    "    StructField(\"departure_delay\",          IntegerType(), True), \\\n",
    "    StructField(\"src_airport\",              StringType(),  True), \\\n",
    "    StructField(\"dest_airport\",             StringType(),  True), \\\n",
    "    StructField(\"distance\",                 IntegerType(), True), \\\n",
    "    StructField(\"taxi_in_time\",             IntegerType(), True), \\\n",
    "    StructField(\"taxi_out_time\",            IntegerType(), True), \\\n",
    "    StructField(\"cancelled\",                StringType(),  True), \\\n",
    "    StructField(\"cancellation_code\",        StringType(),  True), \\\n",
    "    StructField(\"diverted\",                 StringType(),  True), \\\n",
    "    StructField(\"carrier_delay\",            IntegerType(), True), \\\n",
    "    StructField(\"weather_delay\",            IntegerType(), True), \\\n",
    "    StructField(\"nas_delay\",                IntegerType(), True), \\\n",
    "    StructField(\"security_delay\",           IntegerType(), True), \\\n",
    "    StructField(\"late_aircraft_delay\",      IntegerType(), True)\\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert each line into a tuple of features (columns) with the corresponding data type\n",
    "cleaned_data_to_columns = (\n",
    "    cleaned_data.map(lambda l: l.split(\",\"))\n",
    "    .map(lambda cols: \n",
    "         (\n",
    "            int(cols[0])  if cols[ 0] else None,\n",
    "            int(cols[1])  if cols[ 1] else None,\n",
    "            int(cols[2])  if cols[ 2] else None,\n",
    "            int(cols[3])  if cols[ 3] else None,\n",
    "            int(cols[4])  if cols[ 4] else None,\n",
    "            int(cols[5])  if cols[ 5] else None,\n",
    "            int(cols[6])  if cols[ 6] else None,\n",
    "            int(cols[7])  if cols[ 7] else None,\n",
    "            cols[8]       if cols[ 8] else None,\n",
    "            cols[9]       if cols[ 9] else None,\n",
    "            cols[10]      if cols[10] else None,\n",
    "            int(cols[11]) if cols[11] else None,\n",
    "            int(cols[12]) if cols[12] else None,\n",
    "            int(cols[13]) if cols[13] else None,\n",
    "            int(cols[14]) if cols[14] else None,\n",
    "            int(cols[15]) if cols[15] else None,\n",
    "            cols[16]      if cols[16] else None,\n",
    "            cols[17]      if cols[17] else None,\n",
    "            int(cols[18]) if cols[18] else None,\n",
    "            int(cols[19]) if cols[19] else None,\n",
    "            int(cols[20]) if cols[20] else None,\n",
    "            cols[21]      if cols[21] else None,\n",
    "            cols[22]      if cols[22] else None,\n",
    "            cols[23]      if cols[23] else None,\n",
    "            int(cols[24]) if cols[24] else None,\n",
    "            int(cols[25]) if cols[25] else None,\n",
    "            int(cols[26]) if cols[26] else None,\n",
    "            int(cols[27]) if cols[27] else None,\n",
    "            int(cols[28]) if cols[28] else None\n",
    "         ))             \n",
    ")\n",
    "    \n",
    "# create dataframe\n",
    "df = sqlContext.createDataFrame(cleaned_data_to_columns, airline_data_schema)\\\n",
    "    .select(['year', 'month', 'day_of_month', 'day_of_week',\n",
    "            'scheduled_departure_time','scheduled_arrival_time',\n",
    "            'arrival_delay', 'distance', \n",
    "            'src_airport', 'dest_airport', 'carrier'])\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Using the contructed DataFrame, we can answer some simple questions:\n",
    "\n",
    "<ul>\n",
    "<li>How many night flights do we have in our data?</li>\n",
    "<li>How many night flights per unique carrier?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>NOTE:</strong> We define \"night\" to start at 6pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.scheduled_departure_time > 1800].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df.scheduled_departure_time > 1800].groupBy(df.carrier).count().orderBy('count', ascending=0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data exploration\n",
    "Next, we will go over a series of simple queries on our data, to perform exploration and compute statistics using the DataFrame API and SparkSQL. These queries directly map to the questions you need to answer. \n",
    "\n",
    "**NOTE:** finding the right question to ask is difficult! Don't be afraid to complement the questions below, with your own questions that, in your opinion, are valuable ways to inspect data. This can give you extra points!\n",
    "\n",
    "- Basic queries:\n",
    "  - How many unique origin airports?\n",
    "  - How many unique destination airports?\n",
    "  - How many carriers?\n",
    "  - How many flights that have a scheduled departure time later than 18h00?\n",
    "\n",
    "\n",
    "- Statistics on flight volume: this kind of statistics are helpful to reason about delays. Indeed, it is plausible to assume that \"*the more flights in an airport, the higher the probability of delay*\".\n",
    "  - How many flights in each month of the year?\n",
    "  - Is there any relationship between the number of flights and the days of week?\n",
    "  - How many flights in different days of months and in different hours of days?\n",
    "  - Which are the top 20 busiest airports (this depends on inbound and outbound traffic)?\n",
    "  - Which are the top 20 busiest carriers?\n",
    "\n",
    "\n",
    "- Statistics on the fraction of delayed flights\n",
    "  - What is the percentage of delayed flights (over total flights) for different hours of the day?\n",
    "  - Which hours of the day are characterized by the longest flight delay?\n",
    "  - What are the fluctuation of the percentage of delayed flights over different time granularities?\n",
    "  - What is the percentage of delayed flights which depart from one of the top 20 busiest airports?\n",
    "  - What is the percentage of delayed flights which belong to one of the top 20 busiest carriers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Basic queries\n",
    "\n",
    "### Question 2.1\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "How many origin airports? How many destination airports? Discuss the results, indicating any discrepancies or strange facts about the data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of origin airports is: \"\n",
    "      ,df.select(\"src_airport\")\n",
    "      .distinct()\n",
    "      .count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of origin airports is: \"\n",
    "      ,df.select(\"dest_airport\")\n",
    "      .distinct()\n",
    "      .count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "How many unique carriers are present in the data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of carriers in the data is: \"\n",
    "      ,df.select(\"carrier\")\n",
    "      .distinct()\n",
    "      .count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<ul>\n",
    "<li>How many night flights (that is, flights departing later than 6pm or earlier than 6am)?</li>\n",
    "<li>What is the percentage of night flights over the total volume of flights</li>\n",
    "</ul>\n",
    "<br>\n",
    "<strong>NOTE:</strong> Here we're using a more refined definition of what is a night flight, than in our earlier examples.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of night flights in the data is: \"\n",
    "      ,df[(df.scheduled_departure_time>1800) | (df.scheduled_departure_time<600)]\n",
    "      .count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e6d965ad228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The percentage of night flights is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduled_departure_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1800\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduled_departure_time\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    " print(\"The percentage of night flights is:\"\n",
    "       ,df[(df.scheduled_departure_time > 1800) | (df.scheduled_departure_time < 600)]\n",
    "       .count()/df.count(), \"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Flight volume statistics\n",
    "\n",
    "### Question 3.1: \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "How many flights in each month of the year? Plot the changes over months by a line chart and comment the figure.\n",
    "\n",
    "<br>\n",
    "From the result, we can learn the dynamics of flight volume over months. For example, if we only consider flights in 1994 (to start, it's always better to focus on smaller amount of data), we can discuss about which months are most likely to have flights experiencing delays.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2: \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Is there any relationship between the number of flights and the days of the week?  \n",
    "\n",
    "Plot a bar chart and interpret the figure.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<strong>NOTE:</strong> You should study both global aggregates (irrespectively of the month of the year), and monthly aggregates (day statistics, for each different month of the year).\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "By answering this question, we could learn about the importance of the weekend/weekday feature for our predictive task.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "How many flights in different days of months and in different hours of days?  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<strong>NOTE:</strong> Similarly to the previous note, you need to compute both global statistcs and monthly statistics.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Plot  bar charts, and interpret your figures.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Which are the **top 20** busiest airports? Compute this in terms of the inbound, outbound and total number of flights.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5\n",
    "<div class=\"alert alert-info\">\n",
    "Which are the **top 20** busiest carriers? Compute this in terms of number of flights.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 \n",
    "In this series of questions we focus on the computation of statistics about the percentage of delayed flights.\n",
    "\n",
    "### Question 4.1\n",
    "<div class=\"alert alert-info\">\n",
    "What is the percentage of delayed flights for different hours of the day?  \n",
    "\n",
    "Plot a bar chart and interpret the figure.  \n",
    "\n",
    "\n",
    "<div class=\"label label-success\">IMPORTANT!</div> A flight is considered as delayed if it's actual arrival time is more than 15 minutes later than the scheduled arrival time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2\n",
    "<div class=\"alert alert-info\">\n",
    "You will realize that saying *\"at 4 A.M. there is a very low chance of a flight being delayed\"* is not giving you a full picture of the situation. Indeed, it might be true that there is very little probability for an early flight to be delayed, but if it does, the delay might be huge, like 6 hours!  \n",
    "\n",
    "<ul></ul>\n",
    "\n",
    "Then, the question is: **which hours of the day are characterized by the largest mean delay?** (you can consider the median too)  \n",
    "\n",
    "Plot a Bar chart and explain it.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data of year 1994, the flight from 3AM to 4AM often depart earlier than in their schedule. The flights in the morning have less delay then in the afternoon and evening.\n",
    "\n",
    "So, an attentive student should notice here that we have somehow a problem with the definition of delay! Next, we will improve how to represent and visualize data to overcome this problem.\n",
    "\n",
    "**NOTE**: the following piece of code assumes you correctly answered the previous questions. The naming convention of the DataFrames we use next should be clear enough for you to produce them by answering the questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pdf2 = pd.DataFrame(data=mean_delay_per_hour.collect())\n",
    "plt.xlabel(\"Hours\")\n",
    "plt.ylabel(\"Ratio of delay\")\n",
    "plt.title('Figure 6: The radio of delay over hours in day')\n",
    "plt.grid(True,which=\"both\",ls=\"-\")\n",
    "bars = plt.bar(pdf_delay_ratio_per_hour[0], pdf_delay_ratio_per_hour[1], align='center', edgecolor = \"black\")\n",
    "for i in range(0, len(bars)):\n",
    "    color = 'red'\n",
    "    if pdf_mean_delay_per_hour[1][i] < 0:\n",
    "        color = 'lightgreen'\n",
    "    elif pdf_mean_delay_per_hour[1][i] < 2:\n",
    "        color = 'green'\n",
    "    elif pdf_mean_delay_per_hour[1][i] < 4:\n",
    "        color = 'yellow'\n",
    "    elif pdf_mean_delay_per_hour[1][i] < 8:\n",
    "        color = 'orange'\n",
    "\n",
    "    bars[i].set_color(color)\n",
    "        \n",
    "patch1 = mpatches.Patch(color='lightgreen', label='Depart earlier')\n",
    "patch2 = mpatches.Patch(color='green', label='delay < 2 minutes')\n",
    "patch3 = mpatches.Patch(color='yellow', label='delay < 4 minutes')\n",
    "patch4 = mpatches.Patch(color='orange', label='delay < 8 minutes')\n",
    "patch5 = mpatches.Patch(color='red', label='delay >= 8 minutes')\n",
    "\n",
    "plt.legend(handles=[patch1, patch2, patch3, patch4, patch5], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.margins(0.05, 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new figure (Figure 6), we have more information in a single plot. The flights in 3AM to 4AM have very low probability of being delayed, and actually depart earlier than their schedule. In contrast, the flights in the 4PM to 8PM range have higher chances of being delayed: in more than 50% of the cases, the delay is 8 minutes or more.\n",
    "\n",
    "This example shows us that the way representing results are also important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.3\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Plot a bar chart to show the percentage of delayed flights over days in a month. Discuss your findings from the figure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.4\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Plot a bar chart to show the percentage of delayed flights over days in a week. Discuss your findings from the figure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.5\n",
    "<div class=\"alert alert-info\">\n",
    "Plot a bar chart to show the percentage of delayed flights over months in a year. Discuss your findings from the figure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready now to draw some observations from our data, even if we have only looked at data coming from a year worth of flights:\n",
    "\n",
    "- The probability for a flight to be delayed is low at the beginning or at the very end of a given months\n",
    "- Flights on two first weekdays and on the weekend, are less likely to be delayed\n",
    "- May and September are very good months for travelling, as the probability of delay is low (remember we're working on US data. Do you think this is also true in France?)\n",
    "\n",
    "Putting things together, we can have a global picture of the whole year!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_delay = df.withColumn('is_delay', when(df[\"arrival_delay\"] >= 15, 1).otherwise(0))\n",
    "statistic_day = df_with_delay.groupBy(['year','month', 'day_of_month', 'day_of_week'])\\\n",
    "    .agg((func.sum('is_delay')/func.count('*')).alias('delay_ratio'))\n",
    "\n",
    "# assume that we do statistic on year 1994\n",
    "statistic_day = statistic_day\\\n",
    "    .orderBy('year', 'month', 'day_of_month', 'day_of_week')\n",
    "pdf = pd.DataFrame(data=statistic_day.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))                                                             \n",
    "ax = fig.add_subplot(1,1,1)     \n",
    "plt.xlabel(\"Weeks/Months in year\")\n",
    "plt.ylabel(\"Day of weeks (1:Monday -> 7 :Sunday)\")\n",
    "plt.title('Figure 10: The change of number flights over days in year')\n",
    "  \n",
    "rec_size = 0.3\n",
    "from matplotlib.patches import Rectangle\n",
    "import datetime\n",
    "num_days = len(pdf[0])\n",
    "ax.patch.set_facecolor('gray')\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "for i in range(0, num_days):\n",
    "    # extract information from the result\n",
    "    year = pdf[0][i]\n",
    "    month = pdf[1][i]\n",
    "    day_of_month = pdf[2][i]\n",
    "    day_of_week = pdf[3][i]\n",
    "    day_of_year= datetime.date(year=year, month=month, day=day_of_month).timetuple()\n",
    "    week_of_year = datetime.date(year=year, month=month, day=day_of_month).isocalendar()[1]\n",
    "    \n",
    "    # dealing with the week of the previous year\n",
    "    if week_of_year == 52 and month == 1:\n",
    "        week_of_year = 0\n",
    "        \n",
    "    # the coordinate of a day in graph\n",
    "    X = week_of_year*rec_size\n",
    "    Y = day_of_week*rec_size\n",
    "    \n",
    "    # use different colors to show the delay ratio\n",
    "    color = 'white'\n",
    "    if pdf[4][i] <= 0.084:\n",
    "        color = 'lightyellow'\n",
    "    elif pdf[4][i] <= 0.117:\n",
    "        color = 'lightgreen'\n",
    "    elif pdf[4][i] <= 0.152:\n",
    "        color = 'gold'\n",
    "    elif pdf[4][i] <= 0.201:\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        color = 'red'\n",
    "    rect = plt.Rectangle((X - rec_size/2.0, Y - rec_size/2.0), rec_size, rec_size,\n",
    "                      alpha=1, facecolor=color, edgecolor='whitesmoke')\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # drawing boders to separate months\n",
    "    if day_of_month <= 7:\n",
    "        rect2 = plt.Rectangle((X -rec_size/2.0, Y - rec_size/2.0), 0.01, rec_size,\n",
    "                      alpha=1, facecolor='black')\n",
    "        ax.add_patch(rect2)\n",
    "    if day_of_month == 1:\n",
    "        rect2 = plt.Rectangle((X - rec_size/2.0, Y - rec_size/2.0), rec_size, 0.01,\n",
    "                      alpha=1, facecolor='black')\n",
    "        ax.add_patch(rect2)\n",
    "ax.autoscale_view()\n",
    "\n",
    "patch1 = mpatches.Patch(color='lightyellow', label='delay ratio < 8.4%')\n",
    "patch2 = mpatches.Patch(color='lightgreen', label='delay ratio < 11.7%')\n",
    "patch3 = mpatches.Patch(color='gold', label='delay ratio < 15.2%')\n",
    "patch4 = mpatches.Patch(color='orange', label='delay ratio < 20.1%')\n",
    "patch5 = mpatches.Patch(color='red', label='delay ratio >= 20.1%')\n",
    "\n",
    "plt.legend(handles=[patch1, patch2, patch3, patch4, patch5], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.6\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "What is the delay probability for the top 20 busiest airports?  \n",
    "\n",
    "\n",
    "By drawing the flight volume of each airport and the associated delay probability in a single plot, we can observe the relationship between airports, number of flights and the delay.  \n",
    "\n",
    "<div class=\"label label-success\">HINT </div>  Function `<df_colum>.isin(<list>)` helps checking whether a value in column belongs to a list.\n",
    "<ul></ul>\n",
    "<div class=\"label label-success\">SUGGESTION </div>  You can try with different chart type to have a better visualization.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.7\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "What is the percentage of delayed flights which belong to one of the top 20 busiest carriers?  \n",
    "Comment the figure!\n",
    "\n",
    "<ul></ul>\n",
    "<div class=\"label label-success\">SUGGESTION </div>  You can try with different chart type to have a better visualization.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition (bonus) questions\n",
    "\n",
    "### Additional data\n",
    "In the HDFS file system you have used for running the Notebook, you will notice that there are several other years available (in addition to 1994).\n",
    "\n",
    "There are some other datasets that related to this use case:\n",
    "\n",
    "- Airport IATA Codes to City names and Coordinates mapping: http://stat-computing.org/dataexpo/2009/airports.csv\n",
    "- Carrier codes to Full name mapping: http://stat-computing.org/dataexpo/2009/carriers.csv\n",
    "- Information about individual planes: http://stat-computing.org/dataexpo/2009/plane-data.csv\n",
    "- Weather information: http://www.wunderground.com/weather/api/. You can subscribe for free to the developers' API and obtain (at a limited rate) historical weather information in many different formats. Also, to get an idea of the kind of information is available, you can use this link: http://www.wunderground.com/history/\n",
    "\n",
    "### Question 5.1\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Which airports are present in the dataset only as destination airport?\n",
    "\n",
    "Which airport is present only as source airport in the dataset? \n",
    "\n",
    "</div>\n",
    "\n",
    "### Question 5.2\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Which airports have biggest delay probability ?\n",
    "\n",
    "Which cities have largest delay duration in average ?\n",
    "\n",
    "</div>\n",
    "\n",
    "### Question 5.3\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Is there any relationship between weather condition and the delay of flights in 1994 ?\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
